{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you ever open up a file and see something like the following?:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ü17w‰\u0010¸\u0001   é¤   j5¡ 27wP‹5< 7wÿÖjC£Ø17wÿ5 27wÿÖjD£à17w¡ 27wPÿÖ£ä17w3À9\u0005Ø17wtD9\u0005à17wt<9\u0005ä17wt4EèPj\u0018ÿ5Ø17wÿ\u0015\b 7w‹Uì‹M\b‹Eð‰\u0011‹M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "楨杮⁳湯祬洠捡楨敮⁳湵敤獲慴摮⸩吠敨映物瑳挠汯浵⁮獩琠敨戠瑹⁥瑩敳晬‬⁡楢慮祲瘠污敵※桴⁥敳潣摮挠汯浵⁮獩琠敨挠牯敲灳湯楤杮搠捥浩污瘠污敵※桴⁥桴物⁤獩琠敨瘠污敵椠⁮敨慸敤楣慭㭬琠敨映畯瑲⁨獩琠敨挠慨慲瑣牥.甀Ἵ㔀耀\n",
    "\n",
    "桔瑡猧戠瑥整⹲丠睯氠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about importing a text data file only to find two or three strange characters in your first field, which can be resistant to removal? Such as:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ÿþ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you've already experienced problems with character encoding. You may have had many more instances where an encoding problem was more subtle and it escaped your attention.\n",
    "\n",
    "One of the more common challenges for a data wrangler is text data dumps from SQL databases. It is fairly easy to specify a custom delimiter like pipes (|, always a good idea) in the Options dialog of SQL Server Management Studio, but I know of no way to alter the character encoding specified in the table/database design. Thus one will frequently encounter utf-16 encoding in text dumps from SQL databases. Not only are these generally twice as big as they need to be, they cause numerous other challenges as well (see below).\n",
    "\n",
    "One of the biggest changes from Python 2.xx to 3.xx was the added native support of Unicode in 3.xx. Many of the complaints about 3.xx are rooted in the fact that it is frankly so much easier to work just with ASCII. However, the Internet and the international exchange of ideas and text are here to stay and one has to accept that Unicode will overtake ASCII, just as ASCII beat out its competitors back in the dawn of the computer age.\n",
    "\n",
    "First a little setup code to prepare for the examples later in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\dowes\\OneDrive\\Projects\\Encoding')\n",
    "\n",
    "import pandas as pd\n",
    "import codecs as cds\n",
    "\n",
    "def endecode(x, cp):\n",
    "  try:\n",
    "    s = chr(x)\n",
    "    bs = cds.encode(s, 'latin_1')\n",
    "    return cds.decode(bs, cp)\n",
    "  except:\n",
    "    return ''\n",
    "  \n",
    "f  = lambda x : hex(x)[2:].zfill(2)\n",
    "bi = lambda x : bin(x)[2:].zfill(8)\n",
    "ed1252 = lambda x : endecode(x, 'cp1252')\n",
    "ed1253 = lambda x : endecode(x, 'cp1253')\n",
    "ed1254 = lambda x : endecode(x, 'cp1254')\n",
    "ed_ascii = lambda x : endecode(x, 'ascii')\n",
    "ed_latin = lambda x : endecode(x, 'latin_1')\n",
    "ed_8859_2 = lambda x : endecode(x, 'iso8859_2')\n",
    "ed_8859_5 = lambda x : endecode(x, 'iso8859_5')\n",
    "ed_macrom = lambda x : endecode(x, 'mac_roman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As everyone knows, computers work with binary 'bits'. For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0100100001100101011011000110110001101111001000000101011101101111011100100110110001100100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's tough to interpret, even for a computer. So, we need some rules. For example, let's agree to consider bits in blocks of 8, which we'll call bytes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "01001000 01100101 01101100 01101100 01101111 00100000 01010111 01101111 01110010 01101100 01100100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. Now let's have these bytes \"stand for something\", such as text characters (including numeric text characters). Note that 8 binary bits can represent 256 different values ($2^8$ =  256) from 0 to 255. You would think that is plenty of values that we can use to code for (stand for) text characters. In fact, 128 values should be enough. Such was the start of ASCII, a rule system for encoding characters. ASCII came into prominence in the 1960's, beating out alternatives such as BCD (binary coding decimal), FIELDATA (US Army), and EBCDIC (extended BCD, an IBM development for System/360's). Incidentally, it was IBM's adoption of ASCII for the IBM PC that really cemented ASCII's position.\n",
    "\n",
    "Here are most of the 128 ASCII values  \n",
    "- 0-31 are hidden because they code for things only machines understand  \n",
    "- 62-97 are hidden for brevity  \n",
    "- the first column is the byte itself, a binary value  \n",
    "- the second column is the corresponding decimal value  \n",
    "- the third is the value in hexadecimal  \n",
    "- the fourth is the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          bin hex ascii\n",
      "32   00100000  20      \n",
      "33   00100001  21     !\n",
      "34   00100010  22     \"\n",
      "35   00100011  23     #\n",
      "36   00100100  24     $\n",
      "..        ...  ..   ...\n",
      "123  01111011  7b     {\n",
      "124  01111100  7c     |\n",
      "125  01111101  7d     }\n",
      "126  01111110  7e     ~\n",
      "127  01111111  7f     \n",
      "\n",
      "[96 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(128))\n",
    "hx = dec.map(f)\n",
    "bn = dec.map(bi)\n",
    "c_ascii = dec.map(ed_ascii)\n",
    "\n",
    "df = pd.DataFrame(data = {'bin' : bn, 'hex' : hx, 'ascii' : c_ascii})\n",
    "df = df[['bin', 'hex', 'ascii']]\n",
    "print(df[32:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's \"Hello World\" in binary, decimal, and hex bytes, along with ASCII characters. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<xmp>01001000 01100101 01101100 01101100 01101111 00100000 01010111 01101111 01110010 01101100 01100100  \n",
    "72       101      108      108      111      32       87       111      114      108      100  \n",
    "48       65       6C       6C       6F       20       57       6F       72       6C       64  \n",
    "H        e        l        l        o                 W        o        r        l        d</xmp>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not hard to see the appeal of using hex for byte values:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 48 65 6C 6C 6F 20 57 6F 72 6C 64  \n",
    " H  e  l  l  o     W  o  r  l  d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASCII is the acronym for American Standard Code for Information Interchange. As Americans, the inventors felt it was more than adequate. However, Europeans with their umlaut's, superimposed tildes, diacritical marks, etc. needed more. Since Basic ASCII is 7-bit (it doesn't use the first bit in its 128 bytes), going to 8-bits was a way to gain 128 more characters to satisfy the Europeans while retaining backward compatibility with ASCII. Thus Extended ASCII was born - actually a variety of extended ASCII's were born:\n",
    "\n",
    "-IBM - various \"code pages\"  \n",
    "-Apple - MacRoman  \n",
    "-DEC - Multinational Character Set (based on ISO 8859)\n",
    "\n",
    "Eventually, the ISO released a set of standards called \"ISO 8859\" of which \"ISO 8859-1\" (aka Latin-1) is the one used in Western Europe and the USA. Another standard was created for Eastern Europe (ISO 8859-2), one for Cyrillic languages (ISO 8859-5), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          bin hex iso_8859-1 iso_8859-2 iso_8859-5\n",
      "161  10100001  a1          ¡          Ą          Ё\n",
      "162  10100010  a2          ¢          ˘          Ђ\n",
      "163  10100011  a3          £          Ł          Ѓ\n",
      "164  10100100  a4          ¤          ¤          Є\n",
      "165  10100101  a5          ¥          Ľ          Ѕ\n",
      "166  10100110  a6          ¦          Ś          І\n",
      "167  10100111  a7          §          §          Ї\n",
      "168  10101000  a8          ¨          ¨          Ј\n",
      "169  10101001  a9          ©          Š          Љ\n",
      "170  10101010  aa          ª          Ş          Њ\n",
      "171  10101011  ab          «          Ť          Ћ\n",
      "172  10101100  ac          ¬          Ź          Ќ\n",
      "173  10101101  ad          ­          ­          ­\n",
      "174  10101110  ae          ®          Ž          Ў\n",
      "175  10101111  af          ¯          Ż          Џ\n",
      "176  10110000  b0          °          °          А\n",
      "177  10110001  b1          ±          ą          Б\n",
      "178  10110010  b2          ²          ˛          В\n",
      "179  10110011  b3          ³          ł          Г\n",
      "180  10110100  b4          ´          ´          Д\n",
      "181  10110101  b5          µ          ľ          Е\n",
      "182  10110110  b6          ¶          ś          Ж\n",
      "183  10110111  b7          ·          ˇ          З\n",
      "184  10111000  b8          ¸          ¸          И\n",
      "185  10111001  b9          ¹          š          Й\n",
      "186  10111010  ba          º          ş          К\n",
      "187  10111011  bb          »          ť          Л\n",
      "188  10111100  bc          ¼          ź          М\n",
      "189  10111101  bd          ½          ˝          Н\n",
      "190  10111110  be          ¾          ž          О\n",
      "191  10111111  bf          ¿          ż          П\n",
      "192  11000000  c0          À          Ŕ          Р\n",
      "193  11000001  c1          Á          Á          С\n",
      "194  11000010  c2          Â          Â          Т\n",
      "195  11000011  c3          Ã          Ă          У\n",
      "196  11000100  c4          Ä          Ä          Ф\n",
      "197  11000101  c5          Å          Ĺ          Х\n",
      "          bin hex iso_8859-1 iso_8859-2 iso_8859-5\n",
      "228  11100100  e4          ä          ä          ф\n",
      "229  11100101  e5          å          ĺ          х\n",
      "230  11100110  e6          æ          ć          ц\n",
      "231  11100111  e7          ç          ç          ч\n",
      "232  11101000  e8          è          č          ш\n",
      "233  11101001  e9          é          é          щ\n",
      "234  11101010  ea          ê          ę          ъ\n",
      "235  11101011  eb          ë          ë          ы\n",
      "236  11101100  ec          ì          ě          ь\n",
      "237  11101101  ed          í          í          э\n",
      "238  11101110  ee          î          î          ю\n",
      "239  11101111  ef          ï          ď          я\n",
      "240  11110000  f0          ð          đ          №\n",
      "241  11110001  f1          ñ          ń          ё\n",
      "242  11110010  f2          ò          ň          ђ\n",
      "243  11110011  f3          ó          ó          ѓ\n",
      "244  11110100  f4          ô          ô          є\n",
      "245  11110101  f5          õ          ő          ѕ\n",
      "246  11110110  f6          ö          ö          і\n",
      "247  11110111  f7          ÷          ÷          ї\n",
      "248  11111000  f8          ø          ř          ј\n",
      "249  11111001  f9          ù          ů          љ\n",
      "250  11111010  fa          ú          ú          њ\n",
      "251  11111011  fb          û          ű          ћ\n",
      "252  11111100  fc          ü          ü          ќ\n",
      "253  11111101  fd          ý          ý          §\n",
      "254  11111110  fe          þ          ţ          ў\n",
      "255  11111111  ff          ÿ          ˙          џ\n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(128, 256))\n",
    "hx = dec.map(f)\n",
    "bn = dec.map(bi)\n",
    "c8859_1 = dec.map(ed_latin)\n",
    "c8859_2 = dec.map(ed_8859_2)\n",
    "c8859_5 = dec.map(ed_8859_5)\n",
    "\n",
    "df = pd.DataFrame(data = {'bin' : bn, 'hex' : hx, 'iso_8859-1' : c8859_1, 'iso_8859-2' : c8859_2, 'iso_8859-5' : c8859_5})\n",
    "df = df[['bin', 'hex', 'iso_8859-1', 'iso_8859-2', 'iso_8859-5']]\n",
    "df.index = range(128, 256)\n",
    "print(df[33:70])\n",
    "print(df[100:])\n",
    "## again some rows are hidden for the sake of brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clear, a \"code page\" is typically a mapping of all 256 1-byte characters. What I first showed (ASCII) were the first 128 character encodings that are generally common to all encodings. Then I showed the next 128 characters of an extension of ASCII, ISO 8859-1\" (aka Latin-1).\n",
    "\n",
    "Unfortunately, having a code page for Western Europe (ISO 8859-1), another for Eastern Europe (ISO 8859-2), one for Cyrillic languages (ISO 8859-5), etc. makes it difficult to share documents and messages.\n",
    "\n",
    "On top of that, Microsoft has pursued its penchant for proprietary solutions by developing its own set of code pages, cp1250-57 for Windows plus more for MS-DOS. Of these, the Western Europe version cp1252 is the best known. It is a superset of ISO 8859-1 and is also called \"Latin-1\", a common source of confusion.\n",
    "\n",
    "The following shows the variability among these code pages - again, the differences are primarily in the byte values of 128 through 256. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          bin hex iso_8859-1 Win_1252 Win_1253 Win_1254 Mac_Roman\n",
      "128  10000000  80                  €        €        €         Ä\n",
      "129  10000001  81                                              Å\n",
      "130  10000010  82                  ‚        ‚        ‚         Ç\n",
      "131  10000011  83                  ƒ        ƒ        ƒ         É\n",
      "132  10000100  84                  „        „        „         Ñ\n",
      "133  10000101  85          ",
      "        …        …        …         Ö\n",
      "134  10000110  86                  †        †        †         Ü\n",
      "135  10000111  87                  ‡        ‡        ‡         á\n",
      "136  10001000  88                  ˆ                 ˆ         à\n",
      "137  10001001  89                  ‰        ‰        ‰         â\n",
      "138  10001010  8a                  Š                 Š         ä\n",
      "139  10001011  8b                  ‹        ‹        ‹         ã\n",
      "140  10001100  8c                  Œ                 Œ         å\n",
      "141  10001101  8d                                              ç\n",
      "142  10001110  8e                  Ž                           é\n",
      "143  10001111  8f                                              è\n",
      "144  10010000  90                                              ê\n",
      "145  10010001  91                  ‘        ‘        ‘         ë\n",
      "146  10010010  92                  ’        ’        ’         í\n",
      "147  10010011  93                  “        “        “         ì\n",
      "148  10010100  94                  ”        ”        ”         î\n",
      "149  10010101  95                  •        •        •         ï\n",
      "150  10010110  96                  –        –        –         ñ\n",
      "151  10010111  97                  —        —        —         ó\n",
      "152  10011000  98                  ˜                 ˜         ò\n",
      "153  10011001  99                  ™        ™        ™         ô\n",
      "154  10011010  9a                  š                 š         ö\n",
      "155  10011011  9b                  ›        ›        ›         õ\n",
      "156  10011100  9c                  œ                 œ         ú\n",
      "157  10011101  9d                                              ù\n",
      "..        ...  ..        ...      ...      ...      ...       ...\n",
      "168  10101000  a8          ¨        ¨        ¨        ¨         ®\n",
      "169  10101001  a9          ©        ©        ©        ©         ©\n",
      "170  10101010  aa          ª        ª                 ª         ™\n",
      "171  10101011  ab          «        «        «        «         ´\n",
      "172  10101100  ac          ¬        ¬        ¬        ¬         ¨\n",
      "173  10101101  ad          ­        ­        ­        ­         ≠\n",
      "174  10101110  ae          ®        ®        ®        ®         Æ\n",
      "175  10101111  af          ¯        ¯        ―        ¯         Ø\n",
      "176  10110000  b0          °        °        °        °         ∞\n",
      "177  10110001  b1          ±        ±        ±        ±         ±\n",
      "178  10110010  b2          ²        ²        ²        ²         ≤\n",
      "179  10110011  b3          ³        ³        ³        ³         ≥\n",
      "180  10110100  b4          ´        ´        ΄        ´         ¥\n",
      "181  10110101  b5          µ        µ        µ        µ         µ\n",
      "182  10110110  b6          ¶        ¶        ¶        ¶         ∂\n",
      "183  10110111  b7          ·        ·        ·        ·         ∑\n",
      "184  10111000  b8          ¸        ¸        Έ        ¸         ∏\n",
      "185  10111001  b9          ¹        ¹        Ή        ¹         π\n",
      "186  10111010  ba          º        º        Ί        º         ∫\n",
      "187  10111011  bb          »        »        »        »         ª\n",
      "188  10111100  bc          ¼        ¼        Ό        ¼         º\n",
      "189  10111101  bd          ½        ½        ½        ½         Ω\n",
      "190  10111110  be          ¾        ¾        Ύ        ¾         æ\n",
      "191  10111111  bf          ¿        ¿        Ώ        ¿         ø\n",
      "192  11000000  c0          À        À        ΐ        À         ¿\n",
      "193  11000001  c1          Á        Á        Α        Á         ¡\n",
      "194  11000010  c2          Â        Â        Β        Â         ¬\n",
      "195  11000011  c3          Ã        Ã        Γ        Ã         √\n",
      "196  11000100  c4          Ä        Ä        Δ        Ä         ƒ\n",
      "197  11000101  c5          Å        Å        Ε        Å         ≈\n",
      "\n",
      "[70 rows x 7 columns]\n",
      "          bin hex iso_8859-1 Win_1252 Win_1253 Win_1254 Mac_Roman\n",
      "228  11100100  e4          ä        ä        δ        ä         ‰\n",
      "229  11100101  e5          å        å        ε        å         Â\n",
      "230  11100110  e6          æ        æ        ζ        æ         Ê\n",
      "231  11100111  e7          ç        ç        η        ç         Á\n",
      "232  11101000  e8          è        è        θ        è         Ë\n",
      "233  11101001  e9          é        é        ι        é         È\n",
      "234  11101010  ea          ê        ê        κ        ê         Í\n",
      "235  11101011  eb          ë        ë        λ        ë         Î\n",
      "236  11101100  ec          ì        ì        μ        ì         Ï\n",
      "237  11101101  ed          í        í        ν        í         Ì\n",
      "238  11101110  ee          î        î        ξ        î         Ó\n",
      "239  11101111  ef          ï        ï        ο        ï         Ô\n",
      "240  11110000  f0          ð        ð        π        ğ         \n",
      "241  11110001  f1          ñ        ñ        ρ        ñ         Ò\n",
      "242  11110010  f2          ò        ò        ς        ò         Ú\n",
      "243  11110011  f3          ó        ó        σ        ó         Û\n",
      "244  11110100  f4          ô        ô        τ        ô         Ù\n",
      "245  11110101  f5          õ        õ        υ        õ         ı\n",
      "246  11110110  f6          ö        ö        φ        ö         ˆ\n",
      "247  11110111  f7          ÷        ÷        χ        ÷         ˜\n",
      "248  11111000  f8          ø        ø        ψ        ø         ¯\n",
      "249  11111001  f9          ù        ù        ω        ù         ˘\n",
      "250  11111010  fa          ú        ú        ϊ        ú         ˙\n",
      "251  11111011  fb          û        û        ϋ        û         ˚\n",
      "252  11111100  fc          ü        ü        ό        ü         ¸\n",
      "253  11111101  fd          ý        ý        ύ        ı         ˝\n",
      "254  11111110  fe          þ        þ        ώ        ş         ˛\n",
      "255  11111111  ff          ÿ        ÿ                 ÿ         ˇ\n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(128, 256))\n",
    "hx = dec.map(f)\n",
    "bn = dec.map(bi)\n",
    "c8859_1 = dec.map(ed_latin)\n",
    "c1252 = dec.map(ed1252)\n",
    "c1253 = dec.map(ed1253)\n",
    "c1254 = dec.map(ed1254)\n",
    "cMacRom = dec.map(ed_macrom)\n",
    "\n",
    "df = pd.DataFrame(data = {'bin' : bn, 'hex' : hx, 'iso_8859-1' : c8859_1, 'Win_1252' : c1252, 'Win_1253' : c1253, 'Win_1254' : c1254, 'Mac_Roman' : cMacRom})\n",
    "df = df[['bin', 'hex', 'iso_8859-1', 'Win_1252', 'Win_1253', 'Win_1254', 'Mac_Roman']]\n",
    "df.index = range(128, 256)\n",
    "print(df[:70])\n",
    "print(df[100:])\n",
    "## again some rows are hidden for the sake of brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the biggest difference between the ISO standard and the proprietary code pages is the use of the byte values 128 - 159 for characters (in the proprietary code pages). By the way, cp1252 is also known as \"ANSI-1252\" or \"Windows-1252\". It is also called \"Latin-1\" although that term is best reserved for ISO-8859-1.\n",
    "\n",
    "If encoding weren't already getting out of hand, with the advent of the Internet there was a need to consider Russian,  Hindi, Arabic, Hebrew, Korean, etc. alphabets.\n",
    "\n",
    "The solution? multi-byte encoding and and something called unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, unicode is not an encoding system. Unicode is an agreed upon standard of *code points* (numeric values) for characters. How the code points are implemented is not specified. We'll see how implementation can vary when we compare utf-8 and utf-16.\n",
    "\n",
    "Unicode can use as many as 4 bytes, thus potentially providing code points for over 1 million characters. That's enough for all the known human languages and then some (including *Klingon*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utf-8 and utf-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with using 4 bytes (as with utf-32) to code for characters is that most of the time, it is remarkably inefficient. The one-byte representations are sufficient for the vast majority of characters. Going to 4 bytes will unnecessarily quadruple the size of most text documents. The answer is to use *variable-length encoding* that only uses extra bytes when necessary.\n",
    "\n",
    "utf-8 and utf-16 are the two most common implementations of unicode. Google estimates that 84% of the Internet's web pages are utf-8 encoded. utf-8 has at least 1 byte per character and as many as 4. utf-16 starts at 2 bytes and can have as many as 4. This last fact explains why utf-16 encoded text data files will be roughly twice as their utf-8 counterparts. utf-16 also requires the use of a *byte order marker* (BOM) at the beginning of a text file to tell the program what to expect in terms of encoding. The BOM may include information about whether the encoding is *big endian* or *little endian*, a topic beyond the scope of this notebook.\n",
    "\n",
    "utf-8 is identical to ASCII for the first 128 *code points* (values 0 - 127). Thus, simple text encoded in utf-8 can be read by programs designed for ASCII or even extended-ASCII. This can be another source of puzzling problems. Everything can be going swimmingly for you and your software tools when suddenly you start getting uninterpretable character sequences.\n",
    "\n",
    "From data point 128 forward, utf-8 starts using 2, 3, or 4 bytes to encode a character with the *leading byte* signaling how many bytes are to be used for the particular character. The second, third (if needed), and fourth (if needed) *continuation bytes* provide the full definition for the character.\n",
    "\n",
    "Lest you think this is arcane, let me point out a couple of examples from medicine and physics. The inflammatory disease of mucosal surfaces is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beçhet's Disease.\n"
     ]
    }
   ],
   "source": [
    "print('Be' + chr(231) + \"het's Disease.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit of length used to measure distances between atoms or molecules is the:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ångström.\n"
     ]
    }
   ],
   "source": [
    "print(chr(197) + 'ngstr' + chr(246) +'m.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and its symbol is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Å\n"
     ]
    }
   ],
   "source": [
    "print(chr(197))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could have done those examples using cp1252, but frankly it is easier to use the code point numbers and Python's default utf-8 encoding. The point is that scientific and medical terminology includes a number of names, measures, eponyms, etc. that are beyond the encoding range of ASCII or even Latin-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note About Python 3.xx and utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some say that Python 3.xx uses utf-8 internally. That is not quite correct. utf-8 is definitely the default encoding and what you will see at the console or in a text file unless you specify otherwise. But \"internally\", Python is using the numeric values of the unicode **code points**. In the case of the *string* object, which has its own methods, attributes, etc. the string characters are stored as a series of numeric **code point** values.\n",
    "\n",
    "The base chr() function always takes **code point** values as its argument, and this has nothing to do with encodings. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "ç\n",
      "ϧ\n"
     ]
    }
   ],
   "source": [
    "# just as with ASCII\n",
    "print(chr(99))\n",
    "# just as with cp1252\n",
    "print(chr(231))\n",
    "# but here's one the code pages can't do\n",
    "print(chr(999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the unicode implementations use the same **code points** and these all map to the same character, it would not be interesting to see a list of **code points** and **characters** for the different implementations (utf-8, utf-16, utf-32). Instead, it would be interesting to see the differences in bytes.\n",
    "\n",
    "In this next example, we'll look at the binary bytes required for encoding lower end characters (those in the ASCII range). Here utf-8 is more efficient - it is the same as ascii and thus requires only 1 byte. On the other hand, the 2 byte patterns of the utf-16 encodings are very inefficient - the first byte is always NULL (all zeroes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   char utf-8 binary       utf-16be binary\n",
      "50    2     00110010     00000000 00110010\n",
      "51    3     00110011     00000000 00110011\n",
      "52    4     00110100     00000000 00110100\n",
      "53    5     00110101     00000000 00110101\n",
      "54    6     00110110     00000000 00110110\n",
      "55    7     00110111     00000000 00110111\n",
      "56    8     00111000     00000000 00111000\n",
      "57    9     00111001     00000000 00111001\n",
      "58    :     00111010     00000000 00111010\n",
      "59    ;     00111011     00000000 00111011\n",
      "60    <     00111100     00000000 00111100\n",
      "61    =     00111101     00000000 00111101\n",
      "62    >     00111110     00000000 00111110\n",
      "63    ?     00111111     00000000 00111111\n",
      "64    @     01000000     00000000 01000000\n",
      "65    A     01000001     00000000 01000001\n",
      "66    B     01000010     00000000 01000010\n",
      "67    C     01000011     00000000 01000011\n",
      "68    D     01000100     00000000 01000100\n",
      "69    E     01000101     00000000 01000101\n",
      "70    F     01000110     00000000 01000110\n",
      "71    G     01000111     00000000 01000111\n",
      "72    H     01001000     00000000 01001000\n",
      "73    I     01001001     00000000 01001001\n",
      "74    J     01001010     00000000 01001010\n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(50, 75))\n",
    "\n",
    "def binbytes(cp, enc):\n",
    "    bs = cds.encode(chr(cp), enc)\n",
    "    s = '   '\n",
    "    for i in range(0, len(bs)):\n",
    "        s = s + bi(bs[i]) + ' '\n",
    "    return s.rstrip()\n",
    "\n",
    "binbytes_8 = lambda x : binbytes(x, 'utf-8')\n",
    "binbytes_16 = lambda x : binbytes(x, 'utf-16be')\n",
    "# the be stands for 'big endian'\n",
    "\n",
    "bb8 = dec.map(binbytes_8)\n",
    "bb16 = dec.map(binbytes_16)\n",
    "char = dec.map(chr)\n",
    "\n",
    "df = pd.DataFrame(data = {'char' : char, 'utf-8 binary' : bb8, 'utf-16be binary' : bb16})\n",
    "df = df[['char', 'utf-8 binary', 'utf-16be binary']]\n",
    "df.index = range(50, 75)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next code example, we'll look at some \"mid-range\" characters, code points just above the \"extended-ASCII\" range. Note the \"be\" stands for **big endian** (as opposed to \"le\" for **little endian**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    char          utf-8 binary       utf-16be binary\n",
      "350    Ş     11000101 10011110     00000001 01011110\n",
      "351    ş     11000101 10011111     00000001 01011111\n",
      "352    Š     11000101 10100000     00000001 01100000\n",
      "353    š     11000101 10100001     00000001 01100001\n",
      "354    Ţ     11000101 10100010     00000001 01100010\n",
      "355    ţ     11000101 10100011     00000001 01100011\n",
      "356    Ť     11000101 10100100     00000001 01100100\n",
      "357    ť     11000101 10100101     00000001 01100101\n",
      "358    Ŧ     11000101 10100110     00000001 01100110\n",
      "359    ŧ     11000101 10100111     00000001 01100111\n",
      "360    Ũ     11000101 10101000     00000001 01101000\n",
      "361    ũ     11000101 10101001     00000001 01101001\n",
      "362    Ū     11000101 10101010     00000001 01101010\n",
      "363    ū     11000101 10101011     00000001 01101011\n",
      "364    Ŭ     11000101 10101100     00000001 01101100\n",
      "365    ŭ     11000101 10101101     00000001 01101101\n",
      "366    Ů     11000101 10101110     00000001 01101110\n",
      "367    ů     11000101 10101111     00000001 01101111\n",
      "368    Ű     11000101 10110000     00000001 01110000\n",
      "369    ű     11000101 10110001     00000001 01110001\n",
      "370    Ų     11000101 10110010     00000001 01110010\n",
      "371    ų     11000101 10110011     00000001 01110011\n",
      "372    Ŵ     11000101 10110100     00000001 01110100\n",
      "373    ŵ     11000101 10110101     00000001 01110101\n",
      "374    Ŷ     11000101 10110110     00000001 01110110\n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(350, 375))\n",
    "\n",
    "bb8 = dec.map(binbytes_8)\n",
    "bb16 = dec.map(binbytes_16)\n",
    "char = dec.map(chr)\n",
    "\n",
    "df = pd.DataFrame(data = {'char' : char, 'utf-8 binary' : bb8, 'utf-16be binary' : bb16})\n",
    "df = df[['char', 'utf-8 binary', 'utf-16be binary']]\n",
    "df.index = range(350, 375)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next segment, we'll consider *code points* in a much higher range, but one that includes the **Euro sign** (8364). In this range, utf-8 actually requires more bytes (3) than does utf-16. This is because utf-8 uses the *leading byte* to signal how many bytes are used to encode the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     char                   utf-8 binary       utf-16be binary\n",
      "8350    ₞     11100010 10000010 10011110     00100000 10011110\n",
      "8351    ₟     11100010 10000010 10011111     00100000 10011111\n",
      "8352    ₠     11100010 10000010 10100000     00100000 10100000\n",
      "8353    ₡     11100010 10000010 10100001     00100000 10100001\n",
      "8354    ₢     11100010 10000010 10100010     00100000 10100010\n",
      "8355    ₣     11100010 10000010 10100011     00100000 10100011\n",
      "8356    ₤     11100010 10000010 10100100     00100000 10100100\n",
      "8357    ₥     11100010 10000010 10100101     00100000 10100101\n",
      "8358    ₦     11100010 10000010 10100110     00100000 10100110\n",
      "8359    ₧     11100010 10000010 10100111     00100000 10100111\n",
      "8360    ₨     11100010 10000010 10101000     00100000 10101000\n",
      "8361    ₩     11100010 10000010 10101001     00100000 10101001\n",
      "8362    ₪     11100010 10000010 10101010     00100000 10101010\n",
      "8363    ₫     11100010 10000010 10101011     00100000 10101011\n",
      "8364    €     11100010 10000010 10101100     00100000 10101100\n",
      "8365    ₭     11100010 10000010 10101101     00100000 10101101\n",
      "8366    ₮     11100010 10000010 10101110     00100000 10101110\n",
      "8367    ₯     11100010 10000010 10101111     00100000 10101111\n",
      "8368    ₰     11100010 10000010 10110000     00100000 10110000\n",
      "8369    ₱     11100010 10000010 10110001     00100000 10110001\n",
      "8370    ₲     11100010 10000010 10110010     00100000 10110010\n",
      "8371    ₳     11100010 10000010 10110011     00100000 10110011\n",
      "8372    ₴     11100010 10000010 10110100     00100000 10110100\n",
      "8373    ₵     11100010 10000010 10110101     00100000 10110101\n",
      "8374    ₶     11100010 10000010 10110110     00100000 10110110\n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(8350, 8375))\n",
    "\n",
    "bb8 = dec.map(binbytes_8)\n",
    "bb16 = dec.map(binbytes_16)\n",
    "char = dec.map(chr)\n",
    "\n",
    "df = pd.DataFrame(data = {'char' : char, 'utf-8 binary' : bb8, 'utf-16be binary' : bb16})\n",
    "df = df[['char', 'utf-8 binary', 'utf-16be binary']]\n",
    "df.index = range(8350, 8375)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next segment, we'll look at \"high end\" Han (Mandarin) characters. These have code points well past even the most obscure Western characters. In this range, utf-8 and utf-16 encodings both require 4 bytes. (for space reasons, the results are stacked rather than side-by-side)\n",
    "\n",
    "One can see the utf-8 *leading byte* in action in these examples. Below, the first 4 bytes of the utf-8 *leading byte* indicate the encoding is 4-bytes. In the previous example, the first 3 bits of the *leading byte* indicate 3 bytes. In the 350-375 **code point** range, the encodings were 2 bytes and the 2 leading bits of the *leading byte* indicated this. In the first utf-8 example, there was no *leading byte* and utf-8 required only 1 byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       char                            utf-8 binary  \\\n",
      "150350    𤭎     11110000 10100100 10101101 10001110   \n",
      "150351    𤭏     11110000 10100100 10101101 10001111   \n",
      "150352    𤭐     11110000 10100100 10101101 10010000   \n",
      "150353    𤭑     11110000 10100100 10101101 10010001   \n",
      "150354    𤭒     11110000 10100100 10101101 10010010   \n",
      "150355    𤭓     11110000 10100100 10101101 10010011   \n",
      "150356    𤭔     11110000 10100100 10101101 10010100   \n",
      "150357    𤭕     11110000 10100100 10101101 10010101   \n",
      "150358    𤭖     11110000 10100100 10101101 10010110   \n",
      "150359    𤭗     11110000 10100100 10101101 10010111   \n",
      "150360    𤭘     11110000 10100100 10101101 10011000   \n",
      "150361    𤭙     11110000 10100100 10101101 10011001   \n",
      "150362    𤭚     11110000 10100100 10101101 10011010   \n",
      "150363    𤭛     11110000 10100100 10101101 10011011   \n",
      "150364    𤭜     11110000 10100100 10101101 10011100   \n",
      "150365    𤭝     11110000 10100100 10101101 10011101   \n",
      "150366    𤭞     11110000 10100100 10101101 10011110   \n",
      "150367    𤭟     11110000 10100100 10101101 10011111   \n",
      "150368    𤭠     11110000 10100100 10101101 10100000   \n",
      "150369    𤭡     11110000 10100100 10101101 10100001   \n",
      "150370    𤭢     11110000 10100100 10101101 10100010   \n",
      "150371    𤭣     11110000 10100100 10101101 10100011   \n",
      "150372    𤭤     11110000 10100100 10101101 10100100   \n",
      "150373    𤭥     11110000 10100100 10101101 10100101   \n",
      "150374    𤭦     11110000 10100100 10101101 10100110   \n",
      "\n",
      "                               utf-16be binary  \n",
      "150350     11011000 01010010 11011111 01001110  \n",
      "150351     11011000 01010010 11011111 01001111  \n",
      "150352     11011000 01010010 11011111 01010000  \n",
      "150353     11011000 01010010 11011111 01010001  \n",
      "150354     11011000 01010010 11011111 01010010  \n",
      "150355     11011000 01010010 11011111 01010011  \n",
      "150356     11011000 01010010 11011111 01010100  \n",
      "150357     11011000 01010010 11011111 01010101  \n",
      "150358     11011000 01010010 11011111 01010110  \n",
      "150359     11011000 01010010 11011111 01010111  \n",
      "150360     11011000 01010010 11011111 01011000  \n",
      "150361     11011000 01010010 11011111 01011001  \n",
      "150362     11011000 01010010 11011111 01011010  \n",
      "150363     11011000 01010010 11011111 01011011  \n",
      "150364     11011000 01010010 11011111 01011100  \n",
      "150365     11011000 01010010 11011111 01011101  \n",
      "150366     11011000 01010010 11011111 01011110  \n",
      "150367     11011000 01010010 11011111 01011111  \n",
      "150368     11011000 01010010 11011111 01100000  \n",
      "150369     11011000 01010010 11011111 01100001  \n",
      "150370     11011000 01010010 11011111 01100010  \n",
      "150371     11011000 01010010 11011111 01100011  \n",
      "150372     11011000 01010010 11011111 01100100  \n",
      "150373     11011000 01010010 11011111 01100101  \n",
      "150374     11011000 01010010 11011111 01100110  \n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(150350, 150375))\n",
    "\n",
    "bb8 = dec.map(binbytes_8)\n",
    "bb16 = dec.map(binbytes_16)\n",
    "char = dec.map(chr)\n",
    "\n",
    "df = pd.DataFrame(data = {'char' : char, 'utf-8 binary' : bb8, 'utf-16be binary' : bb16})\n",
    "df = df[['char', 'utf-8 binary', 'utf-16be binary']]\n",
    "df.index = range(150350, 150375)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see how utf-8 is much more efficient than utf-16 for typical Western documents. In the traditional ASCII range, utf-8 also requires only 1 byte. On the other hand, it can expand to as many bytes as necessary to code for any **code point**.\n",
    "\n",
    "That does not mean that utf-16 is dead. There is a large range of **code points** for Han characters where utf-8 requires 3 bytes while utf-16 requires only 2. The segment below lists a few of these. I am no expert on Chinese, but I suspect these are the more frequently used characters. Thus, in representing Han characters, utf-16 can be more efficient than utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      char                   utf-8 binary       utf-16be binary\n",
      "18350    䞮     11100100 10011110 10101110     01000111 10101110\n",
      "18351    䞯     11100100 10011110 10101111     01000111 10101111\n",
      "18352    䞰     11100100 10011110 10110000     01000111 10110000\n",
      "18353    䞱     11100100 10011110 10110001     01000111 10110001\n",
      "18354    䞲     11100100 10011110 10110010     01000111 10110010\n",
      "18355    䞳     11100100 10011110 10110011     01000111 10110011\n",
      "18356    䞴     11100100 10011110 10110100     01000111 10110100\n",
      "18357    䞵     11100100 10011110 10110101     01000111 10110101\n",
      "18358    䞶     11100100 10011110 10110110     01000111 10110110\n",
      "18359    䞷     11100100 10011110 10110111     01000111 10110111\n",
      "18360    䞸     11100100 10011110 10111000     01000111 10111000\n",
      "18361    䞹     11100100 10011110 10111001     01000111 10111001\n",
      "18362    䞺     11100100 10011110 10111010     01000111 10111010\n",
      "18363    䞻     11100100 10011110 10111011     01000111 10111011\n",
      "18364    䞼     11100100 10011110 10111100     01000111 10111100\n",
      "18365    䞽     11100100 10011110 10111101     01000111 10111101\n",
      "18366    䞾     11100100 10011110 10111110     01000111 10111110\n",
      "18367    䞿     11100100 10011110 10111111     01000111 10111111\n",
      "18368    䟀     11100100 10011111 10000000     01000111 11000000\n",
      "18369    䟁     11100100 10011111 10000001     01000111 11000001\n",
      "18370    䟂     11100100 10011111 10000010     01000111 11000010\n",
      "18371    䟃     11100100 10011111 10000011     01000111 11000011\n",
      "18372    䟄     11100100 10011111 10000100     01000111 11000100\n",
      "18373    䟅     11100100 10011111 10000101     01000111 11000101\n",
      "18374    䟆     11100100 10011111 10000110     01000111 11000110\n"
     ]
    }
   ],
   "source": [
    "dec = pd.Series(data = range(18350, 18375))\n",
    "\n",
    "bb8 = dec.map(binbytes_8)\n",
    "bb16 = dec.map(binbytes_16)\n",
    "char = dec.map(chr)\n",
    "\n",
    "df = pd.DataFrame(data = {'char' : char, 'utf-8 binary' : bb8, 'utf-16be binary' : bb16})\n",
    "df = df[['char', 'utf-8 binary', 'utf-16be binary']]\n",
    "df.index = range(18350, 18375)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
